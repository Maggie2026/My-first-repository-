Case 1: The Hiring Bot That Plays Favorites

The Setup:
A company’s shiny new AI is screening résumés. Quick, efficient, ruthless. But beneath the polished code? A pattern emerges: women with career gaps are tossed aside like yesterday’s newsprint.

The Clues:

🚨 Bias on the loose: The system is copying old hiring biases, punishing anyone who dared step off the straight career ladder.

🔒 Transparency MIA: Rejected applicants get no explanation. Just silence.

⚖️ Fairness compromised: Brilliant candidates never even make it to the interview.

Case Closed — Fix:
Run bias audits during training. Balance the data. Or, better yet, set explicit fairness rules so a “career break” isn’t mistaken for a “career end.”

Case 2: The Proctoring AI with Shifty Eyes

The Setup:
In schools, webcams are the new hall monitors. An AI stares back at students, tracking every blink and sideways glance. Too many eye movements? The system whispers: “Cheater.”

The Clues:

🎯 Equity breach: Neurodivergent students or those with medical needs get flagged unfairly.

👀 Privacy invasion: Turning bedrooms into surveillance zones isn’t exactly student-friendly.

❓ Accountability gap: Teachers may trust the AI without checking context.

Case Closed — Fix:
Make the AI an assistant, not a judge. Let it flag “unusual” behavior but keep humans in charge of decisions. And scale back on the creepy surveillance — students deserve dignity, not digital detectives breathing down their necks.

🕵️ Inspector’s Closing Note

AI is powerful, but without checks, it can become a suspect itself. My rule of thumb? If the system wouldn’t treat me or my friends fairly — it doesn’t belong in the streets, classrooms, or boardrooms.

Case files stamped: RESOLVED.
